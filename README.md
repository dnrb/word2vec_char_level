# word2vec_char_level

Initial model works on the English corpus.

Finished the initial trainning on Chinese corpus.

Initial optimization of the Chinese model completed.


1. Original Source Files for Chinese Corpus (2019-04-01):

https://dumps.wikimedia.org/zhwiki/20190401/zhwiki-20190401-pages-articles-multistream.xml.bz2 (All Articles)

https://dumps.wikimedia.org/zhwiki/20190401/zhwiki-20190401-pages-articles-multistream1.xml-p1p162886.bz2 (Partial Articles)


2. Data Pre-processing

https://drive.google.com/file/d/1Sr7lO8iFoNUnGkxuIFqQf4fpb00S0G_L/view?usp=sharing (Pre-process Program)

https://drive.google.com/file/d/1Sr7lO8iFoNUnGkxuIFqQf4fpb00S0G_L/view?usp=sharing (Pre-processed Partical Articles)

https://drive.google.com/file/d/1q2bbhhYas8ZCw9jRE09GprjxpOD0mU6r/view?usp=sharing (Pre-processed All Articles)


3. Tensorflow trainings have been performed with GPU or TPU accelerations. In extreme cases, RAM + VRAM usages can be up to 24GB on local machine.

4. Cosine Similarities 
https://medium.com/the-artificial-impostor/preview-developing-modern-chinese-nlp-models-60d4774ebfa7



Credits:

Google Colob

https://github.com/minsuk-heo/python_tutorial/blob/master/data_science/nlp/word2vec_tensorflow.ipynb

https://github.com/savankoradiya/Google-Colab-Tutorial

https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA

https://www.tensorflow.org/tutorials/representation/word2vec

https://www.tensorflow.org/guide/embedding

https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d

https://github.com/minsuk-heo/python_tutorial/blob/master/data_science/nlp/word2vec_tensorflow.ipynb

https://distill.pub/2016/misread-tsne/

https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d

https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d


{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_file(smaller sample).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnrb/word2vec_char_level/blob/master/first_file(smaller_sample).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e6V9UZs3ZgVz",
        "colab_type": "code",
        "outputId": "eba2bd35-7bc2-47cb-8774-d4a3179113aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dnrb/word2vec_char_level.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'word2vec_char_level'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 44 (delta 20), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BYaN6r95aBe9",
        "colab_type": "code",
        "outputId": "05cd6230-edb3-4665-f6ce-49e960b591c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  word2vec_char_level\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c79oIA2xaJZs",
        "colab_type": "code",
        "outputId": "ba787f7f-5bea-400a-e9a1-25c440ad8908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd word2vec_char_level"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/word2vec_char_level\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RTMzA1TIaQW4",
        "colab_type": "code",
        "outputId": "6e3996e1-961c-4bfc-c6ff-6b81faac4c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " first_file.ipynb\t\t     README.md\t  tasa_2.txt\n",
            "'first_file(smaller_sample).ipynb'   tasa_1.txt   tasa_3.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_5CTTY49bs3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Read the input file**"
      ]
    },
    {
      "metadata": {
        "id": "ZeokAW7caRsK",
        "colab_type": "code",
        "outputId": "dc89c35e-8ce9-46f4-8b30-9bb3c2dbf4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "file  = 'tasa_1.txt'\n",
        "filehandle = open(file)\n",
        "lines  = filehandle.readlines()\n",
        "len(lines)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "cDSBuQYLbnId",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Set up the corpus**"
      ]
    },
    {
      "metadata": {
        "id": "p3mAZUAxbkfX",
        "colab_type": "code",
        "outputId": "4d83cd67-ceb1-44c3-a398-3678b383aacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "corpus = lines[:10]\n",
        "print(corpus)\n",
        "len(corpus)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who were the first americans ?\\n', 'many , many years ago , perhaps 35,000 years ago , life was very different than it is today .\\n', 'at that time , the earth was in the grip of the last ice age .\\n', 'there were few people anywhere in the world , and none lived in the americas .\\n', 'people did live in asia , however .\\n', 'and some of them wandered into north america .\\n', 'the firstcomers did not know they had found a new continent .\\n', 'like all ice age peoples , they were hunters .\\n', 'they had to move from place to place in search of their food .\\n', 'sometimes they killed giant elephants called mammoths .\\n']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "QIKrn8PDcAeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = [x[:-2] for x in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snsuFqGTdVO4",
        "colab_type": "code",
        "outputId": "9c7e64ba-184c-459b-e07b-51f5c57aaf1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who were the first americans ', 'many , many years ago , perhaps 35,000 years ago , life was very different than it is today ', 'at that time , the earth was in the grip of the last ice age ', 'there were few people anywhere in the world , and none lived in the americas ', 'people did live in asia , however ', 'and some of them wandered into north america ', 'the firstcomers did not know they had found a new continent ', 'like all ice age peoples , they were hunters ', 'they had to move from place to place in search of their food ', 'sometimes they killed giant elephants called mammoths ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mZkVDs_IgXaq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Gather every words in the corpus**"
      ]
    },
    {
      "metadata": {
        "id": "E_c8-FoNeo9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "words = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    \n",
        "    for word in sentence.split(' '):\n",
        "        \n",
        "        if word not in string.punctuation:\n",
        "            words.append(word)\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrfxCx4AfF_1",
        "colab_type": "code",
        "outputId": "43d1b208-c158-4472-ad13-b85695827d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "ZhA_JrEPh2lv",
        "colab_type": "code",
        "outputId": "f6b5d1e0-eca7-440e-ef5c-0467efca06da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "cell_type": "code",
      "source": [
        "words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who',\n",
              " 'were',\n",
              " 'the',\n",
              " 'first',\n",
              " 'americans',\n",
              " 'many',\n",
              " 'many',\n",
              " 'years',\n",
              " 'ago',\n",
              " 'perhaps',\n",
              " '35,000',\n",
              " 'years',\n",
              " 'ago',\n",
              " 'life',\n",
              " 'was',\n",
              " 'very',\n",
              " 'different',\n",
              " 'than',\n",
              " 'it',\n",
              " 'is',\n",
              " 'today',\n",
              " 'at',\n",
              " 'that',\n",
              " 'time',\n",
              " 'the',\n",
              " 'earth',\n",
              " 'was',\n",
              " 'in',\n",
              " 'the',\n",
              " 'grip',\n",
              " 'of',\n",
              " 'the',\n",
              " 'last',\n",
              " 'ice',\n",
              " 'age',\n",
              " 'there',\n",
              " 'were',\n",
              " 'few',\n",
              " 'people',\n",
              " 'anywhere',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'and',\n",
              " 'none',\n",
              " 'lived',\n",
              " 'in',\n",
              " 'the',\n",
              " 'americas',\n",
              " 'people',\n",
              " 'did',\n",
              " 'live',\n",
              " 'in',\n",
              " 'asia',\n",
              " 'however',\n",
              " 'and',\n",
              " 'some',\n",
              " 'of',\n",
              " 'them',\n",
              " 'wandered',\n",
              " 'into',\n",
              " 'north',\n",
              " 'america',\n",
              " 'the',\n",
              " 'firstcomers',\n",
              " 'did',\n",
              " 'not',\n",
              " 'know',\n",
              " 'they',\n",
              " 'had',\n",
              " 'found',\n",
              " 'a',\n",
              " 'new',\n",
              " 'continent',\n",
              " 'like',\n",
              " 'all',\n",
              " 'ice',\n",
              " 'age',\n",
              " 'peoples',\n",
              " 'they',\n",
              " 'were',\n",
              " 'hunters',\n",
              " 'they',\n",
              " 'had',\n",
              " 'to',\n",
              " 'move',\n",
              " 'from',\n",
              " 'place',\n",
              " 'to',\n",
              " 'place',\n",
              " 'in',\n",
              " 'search',\n",
              " 'of',\n",
              " 'their',\n",
              " 'food',\n",
              " 'sometimes',\n",
              " 'they',\n",
              " 'killed',\n",
              " 'giant',\n",
              " 'elephants',\n",
              " 'called',\n",
              " 'mammoths']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "DEPbxBRsj7OP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convert text to int**"
      ]
    },
    {
      "metadata": {
        "id": "QE906jcGniIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def str_to_int(lst):\n",
        "    counter = 0\n",
        "    new_list = []\n",
        "    existed = {}\n",
        "    \n",
        "    for string in lst:\n",
        "        if string not in existed:\n",
        "            existed[string] = counter\n",
        "            new_list.append(existed[string])\n",
        "            counter += 1\n",
        "        else:\n",
        "            new_list.append(existed[string])\n",
        "            \n",
        "    return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7yRwd5_oGFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2int = str_to_int(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_b6VzicdmLaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f61a66e4-f26c-4519-c45f-960c34b92c75"
      },
      "cell_type": "code",
      "source": [
        "min(word2int)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "D1MCjMpCmObf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46cf6599-67e2-4834-d2a6-86627e48e38e"
      },
      "cell_type": "code",
      "source": [
        "max(word2int)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "dJXwKZXXmTGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Set up the constants**"
      ]
    },
    {
      "metadata": {
        "id": "hxY46f26mWkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_Train = 30\n",
        "n_Voc = max(word2int) + 1\n",
        "n_Corpus = len(word2int)\n",
        "WINDOW_SIZE = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QXJxY79toOti",
        "colab_type": "code",
        "outputId": "25172aa6-f81f-44e3-9346-41983464142b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n_Voc"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "nCJEVyNMeyGd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialize two arrays**"
      ]
    },
    {
      "metadata": {
        "id": "yJQ8yuhhexKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def data_generate(n_Train, n_Voc, n_Corpus, WINDOW_SIZE, word2int):\n",
        "\n",
        "    X = np.zeros((n_Train, n_Voc))\n",
        "    Y = np.zeros((n_Train, n_Voc))\n",
        "\n",
        "    for i in range(n_Train):\n",
        "        j = np.random.choice(n_Corpus, 1)[0]\n",
        "\n",
        "        tw = word2int[j]\n",
        "\n",
        "        Y[i][tw] = 1\n",
        "\n",
        "        for k in range(j - WINDOW_SIZE, j + WINDOW_SIZE + 1):\n",
        "            if k != j and k >= 0 and k < n_Corpus:\n",
        "                cw = word2int[k]\n",
        "                X[i][cw] = 1\n",
        "                \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EDEFW_v7oPXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7fead2be-9e28-4ea2-833a-24a4a845b97c"
      },
      "cell_type": "code",
      "source": [
        "data_generate(n_Train, n_Voc, n_Corpus, WINDOW_SIZE, word2int)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 1., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "9SisXlsDQYhs",
        "colab_type": "code",
        "outputId": "f6b86a57-17c1-4f43-fbd0-19ae1ab1631c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ONE_HOT_DIM = n_Voc\n",
        "\n",
        "# # function to convert numbers to one hot vectors\n",
        "# def to_one_hot_encoding(data_point_index):\n",
        "#     one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
        "#     one_hot_encoding[data_point_index] = 1\n",
        "#     return one_hot_encoding\n",
        "\n",
        "# X = [] # input word\n",
        "# Y = [] # target word\n",
        "\n",
        "# for x, y in zip(df['input'], df['label']):\n",
        "#     X.append(to_one_hot_encoding(word2int[ x ]))\n",
        "#     Y.append(to_one_hot_encoding(word2int[ y ]))\n",
        "\n",
        "# # convert them to numpy arrays\n",
        "# X_train = np.asarray(X)\n",
        "# Y_train = np.asarray(Y)\n",
        "\n",
        "# making placeholders for X_train and Y_train\n",
        "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "\n",
        "# word embedding will be 2 dimension for 2d visualization\n",
        "EMBEDDING_DIM = 2 \n",
        "\n",
        "# hidden layer: which represents word vector eventually\n",
        "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
        "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
        "\n",
        "# output layer\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
        "b2 = tf.Variable(tf.random_normal([1]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
        "\n",
        "# loss function: cross entropy\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
        "\n",
        "# training operation\n",
        "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o2cywu4ZkfWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_sUhj1mHiR_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ]
    },
    {
      "metadata": {
        "id": "QPC4y_0biUKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        },
        "outputId": "0b2cd2f0-6094-4b26-f7dc-c8d9added762"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) \n",
        "\n",
        "iteration = 20000\n",
        "for i in range(iteration):\n",
        "    X, Y = data_generate(n_Train, n_Voc, n_Corpus, WINDOW_SIZE, word2int)\n",
        "    # input is X_train which is one hot encoded word\n",
        "    # label is Y_train which is one hot encoded neighbor word\n",
        "    sess.run(train_op, feed_dict={x: X, y_label: Y})\n",
        "    if i % 100 == 0:\n",
        "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X, y_label: Y}))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 loss is :  8.630628\n",
            "iteration 100 loss is :  4.0226073\n",
            "iteration 200 loss is :  4.3562865\n",
            "iteration 300 loss is :  3.7920294\n",
            "iteration 400 loss is :  3.330387\n",
            "iteration 500 loss is :  3.922671\n",
            "iteration 600 loss is :  3.5120566\n",
            "iteration 700 loss is :  3.4970212\n",
            "iteration 800 loss is :  2.962236\n",
            "iteration 900 loss is :  3.4753752\n",
            "iteration 1000 loss is :  3.0254517\n",
            "iteration 1100 loss is :  3.0300498\n",
            "iteration 1200 loss is :  3.031512\n",
            "iteration 1300 loss is :  3.0072687\n",
            "iteration 1400 loss is :  3.0836694\n",
            "iteration 1500 loss is :  2.6936753\n",
            "iteration 1600 loss is :  2.9291453\n",
            "iteration 1700 loss is :  2.5044515\n",
            "iteration 1800 loss is :  2.756175\n",
            "iteration 1900 loss is :  2.8524954\n",
            "iteration 2000 loss is :  2.8333724\n",
            "iteration 2100 loss is :  2.6630037\n",
            "iteration 2200 loss is :  2.5636947\n",
            "iteration 2300 loss is :  2.476386\n",
            "iteration 2400 loss is :  2.7612786\n",
            "iteration 2500 loss is :  2.8408964\n",
            "iteration 2600 loss is :  3.0211554\n",
            "iteration 2700 loss is :  2.7760026\n",
            "iteration 2800 loss is :  2.5579526\n",
            "iteration 2900 loss is :  2.2753677\n",
            "iteration 3000 loss is :  2.4141366\n",
            "iteration 3100 loss is :  2.6730042\n",
            "iteration 3200 loss is :  2.255976\n",
            "iteration 3300 loss is :  2.6771157\n",
            "iteration 3400 loss is :  2.569763\n",
            "iteration 3500 loss is :  2.1085572\n",
            "iteration 3600 loss is :  2.536082\n",
            "iteration 3700 loss is :  2.5327823\n",
            "iteration 3800 loss is :  2.6684077\n",
            "iteration 3900 loss is :  2.188634\n",
            "iteration 4000 loss is :  2.3341873\n",
            "iteration 4100 loss is :  2.5610375\n",
            "iteration 4200 loss is :  2.509569\n",
            "iteration 4300 loss is :  2.4493306\n",
            "iteration 4400 loss is :  2.3412483\n",
            "iteration 4500 loss is :  2.3682323\n",
            "iteration 4600 loss is :  2.2647429\n",
            "iteration 4700 loss is :  2.5839093\n",
            "iteration 4800 loss is :  2.1726706\n",
            "iteration 4900 loss is :  2.2171912\n",
            "iteration 5000 loss is :  2.24752\n",
            "iteration 5100 loss is :  2.5801706\n",
            "iteration 5200 loss is :  2.5078871\n",
            "iteration 5300 loss is :  2.2510488\n",
            "iteration 5400 loss is :  2.2836616\n",
            "iteration 5500 loss is :  2.122971\n",
            "iteration 5600 loss is :  1.9012197\n",
            "iteration 5700 loss is :  2.35446\n",
            "iteration 5800 loss is :  2.0793462\n",
            "iteration 5900 loss is :  2.1554263\n",
            "iteration 6000 loss is :  2.3885372\n",
            "iteration 6100 loss is :  2.1987214\n",
            "iteration 6200 loss is :  2.2508779\n",
            "iteration 6300 loss is :  2.0239782\n",
            "iteration 6400 loss is :  2.0553534\n",
            "iteration 6500 loss is :  1.8308929\n",
            "iteration 6600 loss is :  2.2444677\n",
            "iteration 6700 loss is :  2.0596735\n",
            "iteration 6800 loss is :  2.157787\n",
            "iteration 6900 loss is :  1.9929166\n",
            "iteration 7000 loss is :  1.8278831\n",
            "iteration 7100 loss is :  2.0910094\n",
            "iteration 7200 loss is :  2.0548077\n",
            "iteration 7300 loss is :  2.319798\n",
            "iteration 7400 loss is :  1.8885996\n",
            "iteration 7500 loss is :  2.2495563\n",
            "iteration 7600 loss is :  1.7932755\n",
            "iteration 7700 loss is :  2.3093905\n",
            "iteration 7800 loss is :  2.011043\n",
            "iteration 7900 loss is :  2.0987155\n",
            "iteration 8000 loss is :  2.0681126\n",
            "iteration 8100 loss is :  2.2111318\n",
            "iteration 8200 loss is :  1.9557546\n",
            "iteration 8300 loss is :  1.9299302\n",
            "iteration 8400 loss is :  1.9198619\n",
            "iteration 8500 loss is :  1.9581223\n",
            "iteration 8600 loss is :  1.8547622\n",
            "iteration 8700 loss is :  2.0769312\n",
            "iteration 8800 loss is :  2.0860991\n",
            "iteration 8900 loss is :  2.105437\n",
            "iteration 9000 loss is :  1.6879287\n",
            "iteration 9100 loss is :  2.2503324\n",
            "iteration 9200 loss is :  1.884625\n",
            "iteration 9300 loss is :  1.66528\n",
            "iteration 9400 loss is :  2.3048275\n",
            "iteration 9500 loss is :  1.5139862\n",
            "iteration 9600 loss is :  2.0806534\n",
            "iteration 9700 loss is :  1.8068937\n",
            "iteration 9800 loss is :  2.239986\n",
            "iteration 9900 loss is :  1.8304365\n",
            "iteration 10000 loss is :  2.0337174\n",
            "iteration 10100 loss is :  1.8939922\n",
            "iteration 10200 loss is :  1.730697\n",
            "iteration 10300 loss is :  1.6000731\n",
            "iteration 10400 loss is :  2.0920815\n",
            "iteration 10500 loss is :  1.8577908\n",
            "iteration 10600 loss is :  1.9392792\n",
            "iteration 10700 loss is :  2.0083332\n",
            "iteration 10800 loss is :  1.8590952\n",
            "iteration 10900 loss is :  1.733257\n",
            "iteration 11000 loss is :  2.020433\n",
            "iteration 11100 loss is :  1.7373885\n",
            "iteration 11200 loss is :  1.8855255\n",
            "iteration 11300 loss is :  1.8175105\n",
            "iteration 11400 loss is :  1.6045173\n",
            "iteration 11500 loss is :  1.8399272\n",
            "iteration 11600 loss is :  1.9195492\n",
            "iteration 11700 loss is :  1.6337173\n",
            "iteration 11800 loss is :  1.947863\n",
            "iteration 11900 loss is :  1.7200522\n",
            "iteration 12000 loss is :  1.8889248\n",
            "iteration 12100 loss is :  1.9730179\n",
            "iteration 12200 loss is :  1.7877171\n",
            "iteration 12300 loss is :  1.9550844\n",
            "iteration 12400 loss is :  1.7111726\n",
            "iteration 12500 loss is :  1.8325391\n",
            "iteration 12600 loss is :  1.7894928\n",
            "iteration 12700 loss is :  1.8422521\n",
            "iteration 12800 loss is :  2.2237294\n",
            "iteration 12900 loss is :  1.6063877\n",
            "iteration 13000 loss is :  1.9439561\n",
            "iteration 13100 loss is :  1.7971377\n",
            "iteration 13200 loss is :  1.802511\n",
            "iteration 13300 loss is :  1.6820335\n",
            "iteration 13400 loss is :  1.6919876\n",
            "iteration 13500 loss is :  1.7268217\n",
            "iteration 13600 loss is :  1.590488\n",
            "iteration 13700 loss is :  1.6239089\n",
            "iteration 13800 loss is :  1.8135953\n",
            "iteration 13900 loss is :  1.9892886\n",
            "iteration 14000 loss is :  2.238992\n",
            "iteration 14100 loss is :  1.7224145\n",
            "iteration 14200 loss is :  1.7037972\n",
            "iteration 14300 loss is :  1.5657575\n",
            "iteration 14400 loss is :  1.8459684\n",
            "iteration 14500 loss is :  1.7550342\n",
            "iteration 14600 loss is :  1.8593218\n",
            "iteration 14700 loss is :  1.7369534\n",
            "iteration 14800 loss is :  1.7988867\n",
            "iteration 14900 loss is :  1.4049128\n",
            "iteration 15000 loss is :  1.5771748\n",
            "iteration 15100 loss is :  1.6995968\n",
            "iteration 15200 loss is :  1.6562426\n",
            "iteration 15300 loss is :  1.490089\n",
            "iteration 15400 loss is :  nan\n",
            "iteration 15500 loss is :  nan\n",
            "iteration 15600 loss is :  nan\n",
            "iteration 15700 loss is :  nan\n",
            "iteration 15800 loss is :  nan\n",
            "iteration 15900 loss is :  nan\n",
            "iteration 16000 loss is :  nan\n",
            "iteration 16100 loss is :  nan\n",
            "iteration 16200 loss is :  nan\n",
            "iteration 16300 loss is :  nan\n",
            "iteration 16400 loss is :  nan\n",
            "iteration 16500 loss is :  nan\n",
            "iteration 16600 loss is :  nan\n",
            "iteration 16700 loss is :  nan\n",
            "iteration 16800 loss is :  nan\n",
            "iteration 16900 loss is :  nan\n",
            "iteration 17000 loss is :  nan\n",
            "iteration 17100 loss is :  nan\n",
            "iteration 17200 loss is :  nan\n",
            "iteration 17300 loss is :  nan\n",
            "iteration 17400 loss is :  nan\n",
            "iteration 17500 loss is :  nan\n",
            "iteration 17600 loss is :  nan\n",
            "iteration 17700 loss is :  nan\n",
            "iteration 17800 loss is :  nan\n",
            "iteration 17900 loss is :  nan\n",
            "iteration 18000 loss is :  nan\n",
            "iteration 18100 loss is :  nan\n",
            "iteration 18200 loss is :  nan\n",
            "iteration 18300 loss is :  nan\n",
            "iteration 18400 loss is :  nan\n",
            "iteration 18500 loss is :  nan\n",
            "iteration 18600 loss is :  nan\n",
            "iteration 18700 loss is :  nan\n",
            "iteration 18800 loss is :  nan\n",
            "iteration 18900 loss is :  nan\n",
            "iteration 19000 loss is :  nan\n",
            "iteration 19100 loss is :  nan\n",
            "iteration 19200 loss is :  nan\n",
            "iteration 19300 loss is :  nan\n",
            "iteration 19400 loss is :  nan\n",
            "iteration 19500 loss is :  nan\n",
            "iteration 19600 loss is :  nan\n",
            "iteration 19700 loss is :  nan\n",
            "iteration 19800 loss is :  nan\n",
            "iteration 19900 loss is :  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AbStdR81hqG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove stop words**"
      ]
    },
    {
      "metadata": {
        "id": "Jkxl31icfHhR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWwiM2zmhxpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# stopwords = nltk.corpus.stopwords.words('english')\n",
        "# cleaned_words = []\n",
        "\n",
        "# for word in words:\n",
        "#     if word not in stopwords:\n",
        "#         cleaned_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}